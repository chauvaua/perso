{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4t6aPBXc7gCE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "#import matlplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.ensemble import  RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "from tensorflow.keras.models import Model, Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D, AveragePooling2D, Conv1D, MaxPooling1D, AveragePooling1D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from tensorflow.keras.callbacks import TensorBoard, CSVLogger,  ModelCheckpoint\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETQRGMtY7gCH"
      },
      "source": [
        "# 1- DATA PREP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAJb-l067gCJ"
      },
      "source": [
        "### Mode de lecture: liste des objets crées"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuOY0Tbp7gCK"
      },
      "source": [
        "Tous les jeux de données suivent ce code:\n",
        "- '_std' = données standardisées avec StandardScaler\n",
        "- '_MM' = données standardisées avec MinMaxScaler\n",
        "- rien de spécifié alors aucune transfo\n",
        "\n",
        "- **y** : catégories des 42 espèces encodées de 0 à 41\n",
        "\n",
        "1- Utiliser la moyenne des réplicats:\n",
        "- **X_train_m**, **X_test_m**\n",
        "- **X_train_m_std**, **X_test_m_std**\n",
        "- **X_train_m_MM**, **X_test_m_MM**\n",
        "- **y_train** et **y_test**\n",
        "\n",
        "2- Utiliser le troisème quantile des réplicats\n",
        "- **X_train_q3**, **X_test_q3**\n",
        "- **X_train_q3_std**, **X_test_q3_std**\n",
        "- **X_train_q3_MM**, **X_test_q3_MM**\n",
        "- **y_train_q3** et **y_test_q3**\n",
        "\n",
        "3 - Utiliser tous les réplicats\n",
        "- **X_test_all**, **X_train_all**\n",
        "- **X_test_all_std**, **X_train_all_std**\n",
        "- **X_test_all_MM**, **X_train_all_MM**\n",
        "- **y_train_all**, **y_test_all**\n",
        "\n",
        "4- Utiliser le Min et le Max des réplicats concatenate ensemble\n",
        "- **X_train_mix**, **X_test_mix**\n",
        "- **X_train_mix_std**, **X_test_mix_std**\n",
        "- **X_train_mix_MM**, **X_test_mix_MM**\n",
        "- **y_train_mix**, **y_test_mix**\n",
        "\n",
        "5 - Prendre en compte le déséquilibre des classes:\n",
        "- **weight** : prendre en compte le déséquilibre des classes dans la fonction de perte"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eH3yy76v7gCM",
        "outputId": "89a450e4-924b-4e8b-fd31-164257422a59"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\anais.rossetto\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:68: FutureWarning: Pass classes=[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
            " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41], y=[4 4 4 ... 9 9 9] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
            "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
          ]
        }
      ],
      "source": [
        "#chargement des données\n",
        "data = np.genfromtxt('spectra-train.csv', delimiter = \";\")\n",
        "label_df = pd.read_csv('meta-train.csv', delimiter = \";\")\n",
        "key = label_df['species'].value_counts().keys().tolist()\n",
        "valeur = label_df['species'].value_counts().tolist()\n",
        "##encode y\n",
        "y = label_df['species']\n",
        "#encode y\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "\n",
        "#pour prendre en compte le déséquilibre des classes\n",
        "#pour calculer les weights\n",
        "weight = class_weight.compute_class_weight('balanced', np.unique(y), y)\n",
        "weight = {i : weight[i] for i in range(42)}\n",
        "\n",
        "# ---- MOYENNE des réplicats\n",
        "#---------------------------------\n",
        "# Création de la matrice avec la moyenne\n",
        "data_mean=np.zeros((len(souches),data.shape[1]))\n",
        "for x in range(len(souches)):\n",
        "    data_mean[x,:]=np.mean(data[x*9:(x+1)*9, :], axis = 0)\n",
        "#data_mean.shape#[346,627]\n",
        "\n",
        "#creation d'une liste avec les indices des réplicats numéro 1:\n",
        "z=0\n",
        "index_R1= [None]* len(souches)\n",
        "for i in range(len(souches)):\n",
        "    #print(z)\n",
        "    index_R1[i] = z\n",
        "    z=z+9\n",
        "\n",
        "# split data_mean\n",
        "X_train_m, X_test_m, y_train, y_test = train_test_split(data_mean, y[index_R1], test_size = 0.15, random_state = 1, stratify = y[index_R1])\n",
        "\n",
        "#standardization pour jeu avec les moyennes des réplicats\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train_m)\n",
        "X_train_m_std = scaler.transform(X_train_m)\n",
        "X_test_m_std = scaler.transform(X_test_m)\n",
        "\n",
        "#standardization MinMax pour jeu avec les moyennes des réplicats\n",
        "scalerMM = MinMaxScaler()\n",
        "scalerMM.fit(X_train_m)\n",
        "X_train_m_MM = scalerMM.transform(X_train_m)\n",
        "X_test_m_MM = scalerMM.transform(X_test_m)\n",
        "\n",
        "\n",
        "# ---- Q3 des réplicats\n",
        "#---------------------------------\n",
        "# Création de la matrice avec le troisième quantile\n",
        "souches = pd.unique(label_df['strain'] )#nb de souches uniques\n",
        "# Q3 pour chaque group de 9\n",
        "data_Q3=np.zeros((len(souches),data.shape[1]))\n",
        "for x in range(len(souches)):\n",
        "    data_Q3[x,:]=np.quantile(data[x*9:(x+1)*9, :], 0.75, axis = 0)\n",
        "#data_Q3.shape#[346,627]\n",
        "\n",
        "# split data_q3\n",
        "X_train_q3, X_test_q3, y_train_q3, y_test_q3 = train_test_split(data_Q3, y[index_R1], test_size = 0.15, random_state = 1, stratify = y[index_R1])\n",
        "\n",
        "#standardization pour jeu avec les moyennes des réplicats\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train_q3)\n",
        "X_train_q3_std = scaler.transform(X_train_q3)\n",
        "X_test_q3_std = scaler.transform(X_test_q3)\n",
        "\n",
        "#standardization MinMax pour jeu avec les moyennes des réplicats\n",
        "scalerMM = MinMaxScaler()\n",
        "scalerMM.fit(X_train_q3)\n",
        "X_train_q3_MM = scalerMM.transform(X_train_q3)\n",
        "X_test_q3_MM = scalerMM.transform(X_test_q3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDVgLykw7gCN"
      },
      "source": [
        "### 3- Travailler avec tous les réplicats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fw78Wbft7gCO"
      },
      "outputs": [],
      "source": [
        "#je recharge tout depuis le debut pour être sûr chargement des données\n",
        "data = np.genfromtxt('spectra-train.csv', delimiter = \";\")\n",
        "label_df = pd.read_csv('meta-train.csv', delimiter = \";\")\n",
        "key = label_df['species'].value_counts().keys().tolist()\n",
        "valeur = label_df['species'].value_counts().tolist()\n",
        "##encode y\n",
        "y = label_df['species']\n",
        "#encode y\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "#ajout d'un index\n",
        "label_df['index'] = np.arange(label_df.shape[0])\n",
        "label_df.iloc[1:20,:]\n",
        "#ajout d'un index idem sur data\n",
        "z = np.arange(label_df.shape[0])\n",
        "#res est data + une colonne ou on inscrit les index origniaux -> utile juste pour le split\n",
        "res = np.concatenate((data,z[:,None]),axis=1)\n",
        "#on crée une liste d'index tous les 9 pour avoir la première ligne de l'indice\n",
        "#cette liste permet de stocker seulement une ligne parmis les 9 réplicats -> but éviter que le train test split sépare des réplicats\n",
        "#initialisation de la liste\n",
        "index9 = []\n",
        "\n",
        "#index des replicat\n",
        "for i in range(label_df.shape[0]):\n",
        "    if i%9==0:\n",
        "        index9.append(i)\n",
        "\n",
        "\n",
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(res[index9,:], y[index9], test_size = 0.15, random_state = 1, stratify = y[index9])\n",
        "\n",
        "#il faut recuperer dans la dernière colonne de X tout les réplicats et les ajouter\n",
        "index_train = (X_train1[:,627])\n",
        "index_train = index_train.astype(int)#besoin transfo int pour utiliser en tant qu'indice\n",
        "index_test = (X_test1[:,627])\n",
        "index_test = index_test.astype(int)\n",
        "\n",
        "#je fais ma première boucle en dehors comme ça évite initialisation et je peux concat dans ma boucle\n",
        "X_train2 = data[index_train[0]:index_train[0]+9,:]\n",
        "y_train2 = y[index_train[0]:index_train[0]+9]\n",
        "X_test2 = data[index_test[0]:index_test[0]+9,:]\n",
        "y_test2 = y[index_test[0]:index_test[0]+9]\n",
        "\n",
        "#ensuite je les utilise pour reformer mes échantillons X_train et X_test depuis data seulement en utilisant les indices\n",
        "for i in range(1,len(index_train)):\n",
        "        X_train2 = np.concatenate((X_train2, data[index_train[i]:index_train[i]+9,:]), axis = 0)\n",
        "        y_train2 = np.concatenate((y_train2, y[index_train[i]:index_train[i]+9]), axis = 0)\n",
        "\n",
        "for i in range(1,len(index_test)):\n",
        "        X_test2 = np.concatenate((X_test2, data[index_test[i]:index_test[i]+9,:]), axis = 0)\n",
        "        y_test2 = np.concatenate((y_test2, y[index_test[i]:index_test[i]+9]), axis = 0)\n",
        "\n",
        "X_train_all = X_train2\n",
        "y_train_all = y_train2\n",
        "X_test_all = X_test2\n",
        "y_test_all = y_test2\n",
        "\n",
        "# ------- !!!!!!!!  Commenter le code ci-dessous, si besoin d'avoir les y NON one HOT Encode\n",
        "# ------------------------------------------------------------------------------------------------\n",
        "#one hot encode y\n",
        "encoder = OneHotEncoder()\n",
        "y2 = y.reshape(-1, 1)\n",
        "y_train_all = y_train_all.reshape(-1, 1)\n",
        "y_test_all = y_test_all.reshape(-1, 1)\n",
        "encoder.fit(y2)\n",
        "y_train_all = encoder.transform(y_train_all).toarray()\n",
        "y_test_all = encoder.transform(y_test_all).toarray()\n",
        "\n",
        "#standardization pour jeu avec les moyennes des réplicats\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train_all)\n",
        "X_train_all_std = scaler.transform(X_train_all)\n",
        "X_test_all_std = scaler.transform(X_test_all)\n",
        "\n",
        "#standardization MinMax pour jeu avec les moyennes des réplicats\n",
        "scalerMM = MinMaxScaler()\n",
        "scalerMM.fit(X_train_all)\n",
        "X_train_all_MM = scalerMM.transform(X_train_all)\n",
        "X_test_all_MM = scalerMM.transform(X_test_all)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuMXe6Tj7gCP"
      },
      "source": [
        "### 4- Travailler avec le Min et le Max de chaque Réplicat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEpkjBDc7gCQ"
      },
      "outputs": [],
      "source": [
        "#creation du vecteur concatenate le Min et le Max\n",
        "\n",
        "# fonction qui boucle sur toutes les souches = de 0 à 9 et créer un sample\n",
        "# calcul la somme de chaque ligne\n",
        "# renvoie l'indice du min et du max\n",
        "# concatene les deux lignes ensemble\n",
        "# concatene chaque nouvelle ligne ensemble\n",
        "\n",
        "# attention !!  cette fonction prend la ligne min et max sans transfo!\n",
        "\n",
        "def MonMinMax(data):\n",
        "\n",
        "    sample = np.zeros((9, data.shape[1]))\n",
        "    for i in range(9):\n",
        "        sample[i, :] = data[i, :]\n",
        "    #on fait la sum par ligne\n",
        "    somme = np.sum(sample, axis = 1)\n",
        "    #recherhe du min et du max\n",
        "    mini = np.argmin(somme)\n",
        "    maxi = np.argmax(somme)\n",
        "\n",
        "    matrice = np.concatenate((sample[mini, :], sample[maxi, :]), axis = 0)\n",
        "    matrice = matrice.reshape(1, len(matrice))#ici on obtient la dim: (1, 1254)\n",
        "\n",
        "    n=9\n",
        "    #tant qu on atteind pas la dernière ligne on boucle\n",
        "    while(n<data.shape[0]):\n",
        "        sample = np.zeros((9, data.shape[1]))\n",
        "        for i in range(9):\n",
        "            sample[i, :] = data[i+n, :]\n",
        "        #on fait la sum par ligne\n",
        "        somme = np.sum(sample, axis = 1)\n",
        "        #recherhe du min et du max\n",
        "        mini = np.argmin(somme)\n",
        "        maxi = np.argmax(somme)\n",
        "        ligne = np.concatenate((sample[mini, :], sample[maxi, :]), axis = 0)\n",
        "        ligne = ligne.reshape(1, len(ligne))\n",
        "        matrice = np.concatenate((matrice, ligne), axis = 0)#(2, 1254)\n",
        "        n = n+ 9\n",
        "    return matrice\n",
        "\n",
        "#transformation de X_train et X _test\n",
        "X_train_mix = MonMinMax(X_train_all)\n",
        "X_test_mix = MonMinMax(X_test_all)\n",
        "temp = int(y_train_all.shape[0]/9) #permet d'avoir 0-9-18 ..\n",
        "y_train_mix = y_train_all[index_R1[0]:index_R1[temp]]\n",
        "temp = int(y_test_all.shape[0]/9)\n",
        "y_test_mix = y_test_all[index_R1[0]:index_R1[temp]]\n",
        "\n",
        "#standardization pour jeu avec les moyennes des réplicats\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train_mix)\n",
        "X_train_mix_std = scaler.transform(X_train_mix)\n",
        "X_test_mix_std = scaler.transform(X_test_mix)\n",
        "\n",
        "#standardization MinMax pour jeu avec les moyennes des réplicats\n",
        "scalerMM = MinMaxScaler()\n",
        "scalerMM.fit(X_train_all)\n",
        "X_train_mix_MM = scalerMM.transform(X_train_mix)\n",
        "X_test_mix_MM = scalerMM.transform(X_test_mix)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}